{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "In this exercise, you need to build a classifier to beat a demo classifier. This document contains three parts:\n",
    "\n",
    "1. **Data preprocessing** describes how to prepare your data for a classifier.\n",
    "2. **Classifier construction** describes how to use [scikit-learn (skearn)](http://scikit-learn.org/stable/) to build classifiers.\n",
    "3. **Exercise** describes your homework (Chinese).\n",
    "\n",
    "## 1. Data preprocessing\n",
    "\n",
    "Data preprocessing is a necessary step to make your data ready for applying a classifier. This section introduces common data preprocessing issues with two examples:\n",
    "\n",
    "1. A cat dataset to dicuss feature normalization.\n",
    "2. A Pokémon dataset to discuss categorical and missing values.\n",
    "\n",
    "See [another document for data preprocessing (Chinese)](/notebooks/unit/data_preprocessing/data_preprocessing.ipynb).\n",
    "\n",
    "### 1.1 Cat classification (feature normalization)\n",
    "\n",
    "The cat image dataset from [Andrew Ng](https://www.coursera.org/specializations/deep-learning) contains pictures of cat and other stuff. In our [cat classifier](#cat_classifier), the feature vector (`X`) represents a 64 x 64 x 3 RGB image and the label (`y`) is either `0` for a non-cat image or `1` for a cat image. The range of R, G and B components is 0~255. A common practice is to normalize the values into 0~1 by dividing 255. In this example, the range of 0~255 is known. However, in general cases the distribution of each feature is unknown. You could try [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which scales features to a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the cat dataset\n",
    "train_X = np.load('train_X.npy')\n",
    "train_Y = np.load('train_Y.npy')\n",
    "test_X = np.load('test_X.npy')\n",
    "test_Y = np.load('test_Y.npy')\n",
    "\n",
    "# flatten the image\n",
    "num_train = train_X.shape[0]\n",
    "num_test = test_X.shape[0]\n",
    "flatten_train_X = train_X.reshape(num_train,-1)\n",
    "flatten_test_X = test_X.reshape(num_test, -1)\n",
    "\n",
    "# normalize each pixel by dividing 255\n",
    "norm_train_X = flatten_train_X / 255\n",
    "norm_test_X = flatten_test_X / 255\n",
    "\n",
    "# construct a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(norm_train_X, train_Y)\n",
    "\n",
    "# use the classifier to predict a image\n",
    "i = 25\n",
    "plt.imshow(test_X[i])\n",
    "print(\"Model prediction: {}\".format(clf.predict(norm_test_X[i].reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pokémon classification (categorical and missing values)\n",
    "\n",
    "The [Pokémon dataset](https://www.kaggle.com/alopez247/pokemon) contains categorical features, such as `Type_1` and `Color`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('pokemon_alopez247.csv') # load the Pokémon dataset\n",
    "df = df.drop(['Number', 'Name', 'Type_2', 'Egg_Group_1', 'Egg_Group_2', 'hasGender', 'Body_Style'], axis=1) # drop unused columns\n",
    "print(df.info())\n",
    "df.head(5) # show the first 5 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three categorical-to-numeircal conversions are introduced here. First, `Type_1` is simply transformed to a unique numerical id, which is usually an incremental number along the dataset. The transformed `Type_1` is used as the label (`y`) to predict. Second, `Color` is [one-hot encoded (Chinese)](https://itw01.com/GJFRE5J.html). Using incremental numbers for color would confuse the classifier. Third, boolean (`True` or `False`) must be converted to integer (`1` or `0`) in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert `Type_1` into numerical\n",
    "mapping = { 'Type_1': {\n",
    "    'Grass': 1,\n",
    "    'Fire': 2,\n",
    "    'Water': 3, 'Ice': 3, # one may merge categories\n",
    "    'Bug': 4,\n",
    "    'Normal': 5,\n",
    "    'Poison': 6, 'Ghost': 6, 'Dark': 6,\n",
    "    'Electric': 7,\n",
    "    'Ground': 8, 'Rock': 8,\n",
    "    'Fairy': 9, 'Dragon': 9, 'Flying': 9,\n",
    "    'Fighting': 10, 'Psychic': 10, 'Steel': 10\n",
    "}}\n",
    "df = df.replace(mapping)\n",
    "\n",
    "# one-hot encoding\n",
    "dummy_df = pd.get_dummies(df['Color'])\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df = df.drop('Color', axis=1)\n",
    "\n",
    "# boolean to int\n",
    "df['isLegendary'] = df['isLegendary'].astype(int)\n",
    "df['hasMegaEvolution'] = df['hasMegaEvolution'].astype(int)\n",
    "\n",
    "df.head(5) # show the first 5 samples after preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pr_Male` contains 77 **missing values**. Many methods could be used to deal with missing value, such as dorpping the feature/sample or filling by zero/column mean. The best method depends on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum()) # Pr_Male has 77 missing values\n",
    "df.dropna(axis=1, inplace=True) # drop columns with missing values\n",
    "df.head(5) # show the first 5 samples after dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thid end of this section, we want to highlight [pandas](https://pandas.pydata.org/), which provides a lot of tools to process data. For exmaple, it is extremely easy to plot feature histograms with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['HP', 'Attack']].plot.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifier construction\n",
    "\n",
    "In python, one can use [scikit-learn (sklearn)](http://scikit-learn.org/stable/) instead of implementing classifier from scratch. sklearn provides various mahcine learning algorithms, including both supervised and unsupervised ones. The above cat classifier shows how to use sklearn for logistic regression. Below is the code for constructing a Pokémon classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_arrays(df):\n",
    "    y = np.array(df['Type_1']) # use `Type_1` as the label to predict\n",
    "    X = np.array(df.iloc[:,1:]) # use other column as features\n",
    "    return X, y\n",
    "train_X, train_y = get_arrays(df_train)\n",
    "test_X, test_y = get_arrays(df_test)\n",
    "\n",
    "scaler = StandardScaler() # normalize\n",
    "svc = SVC(C=5, gamma=0.04) # use SVM as the classifier\n",
    "clf = Pipeline([('scaler', scaler), ('svc', svc)])\n",
    "clf.fit(train_X, train_y) # train the classifier\n",
    "print(\"Accuracy: %.3f\" % (clf.score(train_X, train_y)))\n",
    "print(\"Accuracy: %.3f\" % (clf.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業\n",
    "\n",
    " * 本次作業為建構 [UCI German Credit Data](https://onlinecourses.science.psu.edu/stat857/node/215) 分類模型  \n",
    " * 該資料集包含 1000 筆貸款資料，其中 700 筆正樣本(credit-worthy)以及 300 筆負樣本(not credit-worthy)。每個樣本有 20 個特徵，其中 17 個類別(categorical)特徵，3 個為數值(numeric)特徵，請參考[特徵的說明](https://onlinecourses.science.psu.edu/stat857/node/222)。\n",
    " * 利用準備好的工具，建構分類模型，請參考 [Classifier comparison](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) 選擇機器學習分類器。\n",
    " * 以下為助教提供的資料集介紹及示範分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入準備好的工具\n",
    "import sys\n",
    "sys.path.append('.prepared')\n",
    "import classifier as prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料集\n",
    "\n",
    "UCI German Credit Data 包含 800 筆訓練資料與 200 筆測試資料。每筆由一個 20 維的特徵向量與一個 `1` 或 `0` 的類別組成。其中 `1` 代表正樣本(credit-worthy)；`0` 代表負樣本(not credit-worthy)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prepared.x_train.shape) # 800 筆訓練資料，每筆資料有 20 個特徵\n",
    "print(prepared.x_train[:3])   # 印出前三筆訓練資料的特徵\n",
    "print(prepared.y_train[:3])   # 印出前三筆訓練資料的類別\n",
    "print()\n",
    "print(prepared.x_test.shape)\n",
    "print(prepared.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示範分類器\n",
    "\n",
    "使用訓練資料(`x_train` 與 `y_train`)訓練示範分類器(`demo_clf`)，在訓練資料與測試資料上評估其正確率。請修改下方[動手做](#動手做)的程式碼試著超越這個示範分類器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用訓練資料訓練示範分類器, `demo_clf`\n",
    "demo_clf = prepared.demo(prepared.x_train, prepared.y_train)\n",
    "\n",
    "# 在訓練資料與測試資料上評估其正確率\n",
    "prepared.evaluate(demo_clf)\n",
    "\n",
    "if 'DecisionTreeClassifier' == type(clf).__name__: # if `clf` is a decision tree\n",
    "    prepared.plot(clf) # plot the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動手做\n",
    "\n",
    "修改以下程式區段，試著使用不同的機器學習演算法，建構比示範分類器更好的模型。換句話說，在測試資料上的正確率超過 `0.765`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import classifiers you want to use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: try different classifiers\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "clf = clf.fit(prepared.x_train, prepared.y_train) # train `clf`\n",
    "prepared.evaluate(clf) # evaluate `clf`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
